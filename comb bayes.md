#  基于朴素 Bayes 组合的集成分类器

**摘要** 

朴素Bayes分类器是一种简单有效的机器学习工具。本文用朴素Bayes分类器的原理推导出“朴素Bayes组合”公式，并构造相应的分类器。经过测试，该分类器有较好的分类性能，克服了朴素Bayes分类器精确度差的缺点。

**关键字** 朴素 Bayes 公式



An ensemble classifier based on nb combination



## 导言

朴素Bayes分类器是一种简单易用的分类器，在文本分类方面表现出色。垃圾邮件过滤是它最为成功的商业应用。朴素Bayes分类器建立在条件独立假设的基础上，
$$
c(x)=\arg\max_c p(c|x), p(c|x)\sim \prod_ip(x_i|c)p(c).
$$
而这个假设比较强，通常无法被满足；计算出的后验概率和实际值也相差较大。不过，朴素Bayes分类器却不会因此而受太大影响。实际上，朴素Bayes分类器是一种可加模型，即有下述分解
$$
f(x)=\sum_{i=1}^nf_i(x_i), x=(x_1,x_2,\dots,x_n).
$$
本文提出的改进方法解决了朴素Bayes分类器的两个问题。

1. 通常朴素Bayes分类器要么解决连续型的，要么解决离散型的，即$p(x_i|c)$的分布类型是同一的。而本文的方法不受此限制。
2. 在很多方面神经网络等机器学习算法和Bayes分类器是互补的。本文方法可以以非常简单的方式将两者结合起来。



符号约定：若求和范围不会引起歧义，累加符号就简单写作$\sum_i$，累乘符号写作$\prod_i$。$p(x_i|c)$表示$X$第$i$个分量$X_i$取$x_i$是的条件概率，严格的写法是$P(X_i=x_i|c)$。



## 朴素 Bayes 组合分类器

本节主要推导朴素 Bayes 组合公式，并简述分类器的构造。

### 朴素 Bayes 组合公式

设$x=(x_1,x_2,\cdots,x_m)$，即输入变量被分解成$m$部分，在条件独立假设的基础上，通过简单变形即可得

$$
p(c|x)\sim \prod_ip(x_i|c)p(c)\sim \prod_ip(c|x_i)p(c)^{1-m}.\\
$$

在算法设计上，下面的等价公式会比较好用
$$
\ln p(c|x)\sim \sum_i\ln p(c|x_i) + (1-m)\ln p(c).
$$
作为分量，$x_i$不必是1维；$p(c|x_i)$都是独立计算的，互不干扰，而且也不是每一个都必须用Bayes估计。如果第$i$项使用分类器$f_i$进行估计的，那么
$$
\ln p(c|x)\sim \sum_i\ln f_{i,c}(x_i) + (1-m)\ln p(c),
$$
其中$f_{i,c}$表示$f_i$在$c$上的分量。这就是说，我们只要用不同部分的数据独立训练多个训练分类器，然后简单求和就可以得到一个不错的分类器。这些分类器被称为基分类器（相当于线性代数中的基向量）。这是一种特殊的可加模型，也可以看成一种简单的集成机器学习，即把$f_i(x_i)$看成是$f_i(P_ix)$，其中$P_i$是$x$到$x_i$的投影。我们把这个公式叫做朴素Bayes组合公式，对应的分类器为朴素Bayes组合分类器。

本文的方法最初是为了改进朴素Bayes分类器而提出的，允许任意组合不同的朴素Bayes分类器，如当面对包含连续变量和连续变量的机器学习问题时，可以组合基于Gauss分布和基于多项式的朴素Bayes分类器。但 (4) 式确实不是非要用朴素Bayes分类器计算$p(c|x_i)$，而且实验也支持用其他分类器能大大提高精确度。此时，严格地说它不再是朴素Bayes分类器。



### 分类器的推广

作为加性模型的特殊形式，(5) 的一种简单推广是增加系数：
$$
\ln p(c|x)\sim \sum_i\alpha_i(c)\ln f_{i,c}(x_i) + \beta(c),
$$
这些系数可以通过遗传算法获得，而初始种群可根据(5)合理设置。这个推广将在以后的研究中实现。



该分类器还可以对缺失型数据进行分类，比如，只知道$x=(x_1,x_2,\cdots,x_l)$，则只需
$$
\ln p(c|x)\sim \sum_{i=1}^{l}\ln f_{i,c}(x_i) + (1-l)\ln p(c),
$$


## 算法设计

算法基于公式 (5)。输入变量$X$会被分解成$m$部分，第$i$部分作为第$i$个基分类器的输入；这些分类器的输出则是共同的。根据分割后的样本，分类器被独立训练。具体的流程如下。

### 算法

1. 读入样本集 X,Y
2. 构造朴素Bayes组合分类器，即选取一组基分类器
3. 对输入进行分割，X1, X2,...,Xm
4. 模型i拟合Xi,Y，i=1,2,...,m；
5. 利用朴素Bayes组合公式(5)，对任意输入$x$计算概率值$p(c|x)$
6. 根据概率值给出预测值



其中第3步根据属性的数据类型进行分割，基本原则是分离离散与连续变量；第4步可以并行计算，可获得较快的速度。

*注* 离散型和连续型的分别通常是相对的。一般多数观测值的频率都比较小时，就应该被看作连续变量。



### 实现

算法的实现也非常简便。本文采用Python实现。主要依赖scikit-learn机器学习库。本文算法基于和朴素Bayes分类器一样的公式，因此它的实现只需继承scikit-learn提供的实现朴素Bayes算法的抽象类即可。



## 实验

实验数据来自CCF人工智能竞赛平台https://www.datafountain.cn/competitions/337。为了使它成为一个分类问题，已经把输出变量分成三类。总共5000条数据，抽出30%作为测试数据。

根据数据，输入变量被大致分为三个部分：0-1型，整数型，实数型。关键的原则依然是看数据的频数。选取适合的基分类器。集成的分类器将和这些基分类器（单独使用）进行比较。



实验结果复合预期。无论耗时还是精确度，朴素Bayes组合分类器都是介于朴素Bayes分类器和其他分类器之间。该算法适用于那些允许牺牲一定精确度来节省时间的分类问题。



## 结语

本文利用本朴素Bayes组合公式设计出一种新的分类器，它是一种非常简便的集成机器学习方法。实验结果表明，算法在不是去太多精确度的情况下，可以提高算法性能。如果需要在精确度和计算时间之间权衡，那么可以使用本算法。

如果基分类器都是朴素Bayes分类器，那么朴素Bayes组合的结果当然也是朴素Bayes分类器。这样就可以轻易组合出能处理混合不同分布类型的数据集，如本文中的实验数据，存在至少三种类型的分布。因为基分类器并不限于朴素Bayes分类器，所以对条件独立性的假设的依赖也减轻了，从而提高了精确度。

由于，本分类器以依然保留了不完全的条件独立假设，因此精确度的提升也是有限的，不能和某些成熟的算法的竞争。但是，它的设计灵活而简单，并具有并行性，和那些“成熟算法”相比，适当的设置可以大大缩减计算时间，而不会显著降低精确度。因此，这个算法适合那些对计算时间有较高要求的领域。


## 文献

[1]Hastie, Trevor, Tibshirani. THE ELEMENTS OF STATISTICAL LEARNING: DATA MINING, INFERENCE, AND PREDICTION, SECOND EDITION[M]. Springer, 2001.

[2] 李航. 统计学习方法[M]. 清华大学出版社, 北京, 2012.

[3] 刘长龙. 从机器学习到深度学习 基于scikit-learn与TensorFlow的高效开发实战[M], 电子工业出版社,2019.



Zhang H . The Optimality of Naive Bayes[C]// Seventeenth International Florida Artificial Intelligence Research Society Conference. 2004.




I Rish, An Empirical Study of the Naïve Bayes Classifier[J], IJCAI 2001 Work Empir Methods Artif Intell, 2001,1(3).